{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_GaqKgQjSk"
      },
      "source": [
        "# 3 Convolutional Neural Network for Skin Lesion Classification using PyTorch\n",
        "\n",
        "In this tutorial, we'll build a Convolutional Neural Network (CNN) using PyTorch to classify skin lesions from the HAM10000 dataset. This dataset contains 10,000 dermatoscopic images of common pigmented skin lesions across seven diagnostic categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsCYY489mFvB"
      },
      "source": [
        "## 3.1 Downloading the dataset\n",
        "\n",
        "The zipped dataset is about 3 GB in size, so the download may take a few minutes..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\n",
            "  0 5324M    0 80462    0     0   141k      0 10:43:21 --:--:-- 10:43:21  141k\n",
            "  0 5324M    0 23.8M    0     0  15.3M      0  0:05:47  0:00:01  0:05:46 23.7M\n",
            "  1 5324M    1 68.3M    0     0  26.5M      0  0:03:20  0:00:02  0:03:18 33.9M\n",
            "  2 5324M    2  111M    0     0  31.4M      0  0:02:49  0:00:03  0:02:46 37.2M\n",
            "  2 5324M    2  157M    0     0  34.5M      0  0:02:34  0:00:04  0:02:30 39.2M\n",
            "  3 5324M    3  201M    0     0  36.2M      0  0:02:27  0:00:05  0:02:22 40.2M\n",
            "  4 5324M    4  246M    0     0  37.5M      0  0:02:21  0:00:06  0:02:15 44.4M\n",
            "  5 5324M    5  290M    0     0  38.4M      0  0:02:18  0:00:07  0:02:11 44.5M\n",
            "  6 5324M    6  335M    0     0  39.1M      0  0:02:15  0:00:08  0:02:07 44.6M\n",
            "  7 5324M    7  373M    0     0  38.9M      0  0:02:16  0:00:09  0:02:07 43.0M\n",
            "  7 5324M    7  413M    0     0  39.1M      0  0:02:15  0:00:10  0:02:05 42.4M\n",
            "  8 5324M    8  459M    0     0  39.7M      0  0:02:13  0:00:11  0:02:02 42.5M\n",
            "  9 5324M    9  502M    0     0  39.9M      0  0:02:13  0:00:12  0:02:01 42.3M\n",
            " 10 5324M   10  544M    0     0  40.1M      0  0:02:12  0:00:13  0:01:59 41.8M\n",
            " 11 5324M   11  588M    0     0  40.4M      0  0:02:11  0:00:14  0:01:57 43.2M\n",
            " 11 5324M   11  632M    0     0  40.6M      0  0:02:10  0:00:15  0:01:55 43.7M\n",
            " 12 5324M   12  672M    0     0  40.6M      0  0:02:11  0:00:16  0:01:55 42.7M\n",
            " 13 5324M   13  714M    0     0  40.7M      0  0:02:10  0:00:17  0:01:53 42.5M\n",
            " 14 5324M   14  758M    0     0  40.8M      0  0:02:10  0:00:18  0:01:52 42.8M\n",
            " 15 5324M   15  802M    0     0  41.0M      0  0:02:09  0:00:19  0:01:50 42.7M\n",
            " 15 5324M   15  845M    0     0  41.1M      0  0:02:09  0:00:20  0:01:49 42.5M\n",
            " 16 5324M   16  885M    0     0  41.0M      0  0:02:09  0:00:21  0:01:48 42.4M\n",
            " 17 5324M   17  924M    0     0  40.9M      0  0:02:09  0:00:22  0:01:47 41.8M\n",
            " 18 5324M   18  967M    0     0  41.0M      0  0:02:09  0:00:23  0:01:46 41.8M\n",
            " 19 5324M   19 1011M    0     0  41.2M      0  0:02:09  0:00:24  0:01:45 41.8M\n",
            " 19 5324M   19 1043M    0     0  40.8M      0  0:02:10  0:00:25  0:01:45 39.6M\n",
            " 20 5324M   20 1085M    0     0  40.8M      0  0:02:10  0:00:26  0:01:44 40.0M\n",
            " 21 5324M   21 1127M    0     0  40.9M      0  0:02:10  0:00:27  0:01:43 40.8M\n",
            " 22 5324M   22 1172M    0     0  41.0M      0  0:02:09  0:00:28  0:01:41 40.9M\n",
            " 22 5324M   22 1217M    0     0  41.1M      0  0:02:09  0:00:29  0:01:40 41.1M\n",
            " 23 5324M   23 1261M    0     0  41.2M      0  0:02:09  0:00:30  0:01:39 43.5M\n",
            " 24 5324M   24 1304M    0     0  41.3M      0  0:02:08  0:00:31  0:01:37 43.6M\n",
            " 25 5324M   25 1348M    0     0  41.4M      0  0:02:08  0:00:32  0:01:36 44.0M\n",
            " 26 5324M   26 1388M    0     0  41.3M      0  0:02:08  0:00:33  0:01:35 43.1M\n",
            " 26 5324M   26 1433M    0     0  41.4M      0  0:02:08  0:00:34  0:01:34 43.2M\n",
            " 27 5324M   27 1476M    0     0  41.5M      0  0:02:08  0:00:35  0:01:33 43.0M\n",
            " 28 5324M   28 1521M    0     0  41.6M      0  0:02:07  0:00:36  0:01:31 43.5M\n",
            " 29 5324M   29 1561M    0     0  41.5M      0  0:02:08  0:00:37  0:01:31 42.6M\n",
            " 30 5324M   30 1606M    0     0  41.6M      0  0:02:07  0:00:38  0:01:29 43.5M\n",
            " 30 5324M   30 1649M    0     0  41.6M      0  0:02:07  0:00:39  0:01:28 43.1M\n",
            " 31 5324M   31 1693M    0     0  41.7M      0  0:02:07  0:00:40  0:01:27 43.3M\n",
            " 32 5324M   32 1735M    0     0  41.7M      0  0:02:07  0:00:41  0:01:26 42.8M\n",
            " 33 5324M   33 1778M    0     0  41.8M      0  0:02:07  0:00:42  0:01:25 43.4M\n",
            " 34 5324M   34 1815M    0     0  41.6M      0  0:02:07  0:00:43  0:01:24 41.9M\n",
            " 34 5324M   34 1857M    0     0  41.6M      0  0:02:07  0:00:44  0:01:23 41.6M\n",
            " 35 5324M   35 1902M    0     0  41.7M      0  0:02:07  0:00:45  0:01:22 41.8M\n",
            " 36 5324M   36 1937M    0     0  41.6M      0  0:02:07  0:00:46  0:01:21 40.4M\n",
            " 37 5324M   37 1981M    0     0  41.6M      0  0:02:07  0:00:47  0:01:20 40.5M\n",
            " 38 5324M   38 2023M    0     0  41.6M      0  0:02:07  0:00:48  0:01:19 41.5M\n",
            " 38 5324M   38 2060M    0     0  41.5M      0  0:02:08  0:00:49  0:01:19 40.5M\n",
            " 39 5324M   39 2105M    0     0  41.6M      0  0:02:07  0:00:50  0:01:17 40.5M\n",
            " 40 5324M   40 2145M    0     0  41.6M      0  0:02:07  0:00:51  0:01:16 41.4M\n",
            " 41 5324M   41 2188M    0     0  41.6M      0  0:02:07  0:00:52  0:01:15 41.2M\n",
            " 41 5324M   41 2231M    0     0  41.6M      0  0:02:07  0:00:53  0:01:14 41.6M\n",
            " 42 5324M   42 2276M    0     0  41.7M      0  0:02:07  0:00:54  0:01:13 43.1M\n",
            " 43 5324M   43 2320M    0     0  41.7M      0  0:02:07  0:00:55  0:01:12 42.9M\n",
            " 44 5324M   44 2362M    0     0  41.7M      0  0:02:07  0:00:56  0:01:11 43.4M\n",
            " 45 5324M   45 2404M    0     0  41.7M      0  0:02:07  0:00:57  0:01:10 43.2M\n",
            " 45 5324M   45 2445M    0     0  41.7M      0  0:02:07  0:00:58  0:01:09 42.7M\n",
            " 46 5324M   46 2484M    0     0  41.7M      0  0:02:07  0:00:59  0:01:08 41.6M\n",
            " 47 5324M   47 2529M    0     0  41.7M      0  0:02:07  0:01:00  0:01:07 41.8M\n",
            " 48 5324M   48 2563M    0     0  41.6M      0  0:02:07  0:01:01  0:01:06 40.3M\n",
            " 48 5324M   48 2605M    0     0  41.6M      0  0:02:07  0:01:02  0:01:05 40.1M\n",
            " 49 5324M   49 2643M    0     0  41.5M      0  0:02:08  0:01:03  0:01:05 39.4M\n",
            " 50 5324M   50 2686M    0     0  41.6M      0  0:02:07  0:01:04  0:01:03 40.4M\n",
            " 51 5324M   51 2730M    0     0  41.6M      0  0:02:07  0:01:05  0:01:02 40.2M\n",
            " 52 5324M   52 2773M    0     0  41.6M      0  0:02:07  0:01:06  0:01:01 41.9M\n",
            " 52 5324M   52 2813M    0     0  41.6M      0  0:02:07  0:01:07  0:01:00 41.3M\n",
            " 53 5324M   53 2855M    0     0  41.6M      0  0:02:07  0:01:08  0:00:59 42.1M\n",
            " 54 5324M   54 2897M    0     0  41.6M      0  0:02:07  0:01:09  0:00:58 42.2M\n",
            " 55 5324M   55 2936M    0     0  41.5M      0  0:02:07  0:01:10  0:00:57 40.9M\n",
            " 55 5324M   55 2979M    0     0  41.6M      0  0:02:07  0:01:11  0:00:56 41.0M\n",
            " 56 5324M   56 3020M    0     0  41.6M      0  0:02:07  0:01:12  0:00:55 41.6M\n",
            " 57 5324M   57 3064M    0     0  41.6M      0  0:02:07  0:01:13  0:00:54 42.1M\n",
            " 58 5324M   58 3100M    0     0  41.5M      0  0:02:08  0:01:14  0:00:54 40.0M\n",
            " 59 5324M   59 3144M    0     0  41.6M      0  0:02:07  0:01:15  0:00:52 41.8M\n",
            " 59 5324M   59 3183M    0     0  41.5M      0  0:02:08  0:01:16  0:00:52 40.8M\n",
            " 60 5324M   60 3221M    0     0  41.5M      0  0:02:08  0:01:17  0:00:51 40.3M\n",
            " 61 5324M   61 3265M    0     0  41.5M      0  0:02:08  0:01:18  0:00:50 40.2M\n",
            " 62 5324M   62 3309M    0     0  41.5M      0  0:02:08  0:01:19  0:00:49 42.1M\n",
            " 62 5324M   62 3351M    0     0  41.5M      0  0:02:07  0:01:20  0:00:47 41.3M\n",
            " 63 5324M   63 3395M    0     0  41.6M      0  0:02:07  0:01:21  0:00:46 42.4M\n",
            " 64 5324M   64 3436M    0     0  41.6M      0  0:02:07  0:01:22  0:00:45 42.9M\n",
            " 65 5324M   65 3478M    0     0  41.6M      0  0:02:07  0:01:23  0:00:44 42.4M\n",
            " 66 5324M   66 3522M    0     0  41.6M      0  0:02:07  0:01:24  0:00:43 42.6M\n",
            " 66 5324M   66 3565M    0     0  41.6M      0  0:02:07  0:01:25  0:00:42 42.9M\n",
            " 67 5324M   67 3609M    0     0  41.7M      0  0:02:07  0:01:26  0:00:41 42.7M\n",
            " 68 5324M   68 3652M    0     0  41.7M      0  0:02:07  0:01:27  0:00:40 43.3M\n",
            " 69 5324M   69 3695M    0     0  41.7M      0  0:02:07  0:01:28  0:00:39 43.5M\n",
            " 70 5324M   70 3739M    0     0  41.7M      0  0:02:07  0:01:29  0:00:38 43.4M\n",
            " 70 5324M   70 3779M    0     0  41.7M      0  0:02:07  0:01:30  0:00:37 42.6M\n",
            " 71 5324M   71 3823M    0     0  41.7M      0  0:02:07  0:01:31  0:00:36 42.8M\n",
            " 72 5324M   72 3869M    0     0  41.8M      0  0:02:07  0:01:32  0:00:35 43.2M\n",
            " 73 5324M   73 3910M    0     0  41.7M      0  0:02:07  0:01:33  0:00:34 42.7M\n",
            " 74 5324M   74 3952M    0     0  41.7M      0  0:02:07  0:01:34  0:00:33 42.5M\n",
            " 75 5324M   75 3996M    0     0  41.8M      0  0:02:07  0:01:35  0:00:32 43.4M\n",
            " 75 5324M   75 4037M    0     0  41.8M      0  0:02:07  0:01:36  0:00:31 42.7M\n",
            " 76 5324M   76 4072M    0     0  41.7M      0  0:02:07  0:01:37  0:00:30 40.6M\n",
            " 77 5324M   77 4112M    0     0  41.7M      0  0:02:07  0:01:38  0:00:29 40.6M\n",
            " 77 5324M   77 4151M    0     0  41.7M      0  0:02:07  0:01:39  0:00:28 39.9M\n",
            " 78 5324M   78 4193M    0     0  41.7M      0  0:02:07  0:01:40  0:00:27 39.4M\n",
            " 79 5324M   79 4237M    0     0  41.7M      0  0:02:07  0:01:41  0:00:26 40.0M\n",
            " 80 5324M   80 4280M    0     0  41.7M      0  0:02:07  0:01:42  0:00:25 41.5M\n",
            " 81 5324M   81 4323M    0     0  41.7M      0  0:02:07  0:01:43  0:00:24 42.2M\n",
            " 82 5324M   82 4367M    0     0  41.7M      0  0:02:07  0:01:44  0:00:23 43.0M\n",
            " 82 5324M   82 4407M    0     0  41.7M      0  0:02:07  0:01:45  0:00:22 42.8M\n",
            " 83 5324M   83 4447M    0     0  41.7M      0  0:02:07  0:01:46  0:00:21 41.9M\n",
            " 84 5324M   84 4489M    0     0  41.7M      0  0:02:07  0:01:47  0:00:20 41.7M\n",
            " 85 5324M   85 4533M    0     0  41.7M      0  0:02:07  0:01:48  0:00:19 41.9M\n",
            " 85 5324M   85 4571M    0     0  41.7M      0  0:02:07  0:01:49  0:00:18 40.7M\n",
            " 86 5324M   86 4614M    0     0  41.7M      0  0:02:07  0:01:50  0:00:17 41.1M\n",
            " 87 5324M   87 4654M    0     0  41.7M      0  0:02:07  0:01:51  0:00:16 41.3M\n",
            " 88 5324M   88 4699M    0     0  41.7M      0  0:02:07  0:01:52  0:00:15 41.9M\n",
            " 89 5324M   89 4739M    0     0  41.7M      0  0:02:07  0:01:53  0:00:14 41.2M\n",
            " 89 5324M   89 4781M    0     0  41.7M      0  0:02:07  0:01:54  0:00:13 42.2M\n",
            " 90 5324M   90 4826M    0     0  41.7M      0  0:02:07  0:01:55  0:00:12 42.5M\n",
            " 91 5324M   91 4869M    0     0  41.7M      0  0:02:07  0:01:56  0:00:11 43.1M\n",
            " 92 5324M   92 4913M    0     0  41.7M      0  0:02:07  0:01:57  0:00:10 42.8M\n",
            " 92 5324M   92 4949M    0     0  41.7M      0  0:02:07  0:01:58  0:00:09 41.8M\n",
            " 93 5324M   93 4985M    0     0  41.7M      0  0:02:07  0:01:59  0:00:08 40.7M\n",
            " 94 5324M   94 5021M    0     0  41.6M      0  0:02:07  0:02:00  0:00:07 38.9M\n",
            " 94 5324M   94 5052M    0     0  41.5M      0  0:02:08  0:02:01  0:00:07 36.6M\n",
            " 95 5324M   95 5097M    0     0  41.5M      0  0:02:08  0:02:02  0:00:06 36.7M\n",
            " 96 5324M   96 5141M    0     0  41.6M      0  0:02:07  0:02:03  0:00:04 38.4M\n",
            " 97 5324M   97 5184M    0     0  41.6M      0  0:02:07  0:02:04  0:00:03 39.8M\n",
            " 98 5324M   98 5223M    0     0  41.6M      0  0:02:07  0:02:05  0:00:02 40.3M\n",
            " 98 5324M   98 5264M    0     0  41.6M      0  0:02:07  0:02:06  0:00:01 42.3M\n",
            " 99 5324M   99 5305M    0     0  41.5M      0  0:02:08  0:02:07  0:00:01 41.3M\n",
            "100 5324M  100 5324M    0     0  41.5M      0  0:02:08  0:02:08 --:--:-- 41.0M\n"
          ]
        }
      ],
      "source": [
        "# ! curl -L -o skin-cancer-mnist-ham10000.zip https://www.kaggle.com/api/v1/datasets/download/kmader/skin-cancer-mnist-ham10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MkDHMXESRBQ"
      },
      "outputs": [],
      "source": [
        "# !unzip skin-cancer-mnist-ham10000.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Importing and initial setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AppNRcbvQi5Q"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages (run this if needed)\n",
        "# !pip install torch torchvision pandas matplotlib seaborn scikit-learn pillow tqdm\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhGgRKLyQr8d"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyAjuNkQ0vd"
      },
      "source": [
        "## 3.3 Working with image data\n",
        "\n",
        "### 3.3.1 Exploring the dataset\n",
        "\n",
        "The HAM10000 dataset consists of 10,015 dermatoscopic images across 7 different categories:\n",
        "- Melanocytic nevi (nv)\n",
        "- Melanoma (mel)\n",
        "- Benign keratosis-like lesions (bkl)\n",
        "- Basal cell carcinoma (bcc)\n",
        "- Actinic keratoses (akiec)\n",
        "- Vascular lesions (vasc)\n",
        "- Dermatofibroma (df)\n",
        "\n",
        "Let's first explore the metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzX8_yleQr-9"
      },
      "outputs": [],
      "source": [
        "# Load the metadata\n",
        "metadata = pd.read_csv('HAM10000_metadata.csv')\n",
        "\n",
        "# Display first few rows\n",
        "print(metadata.head())\n",
        "\n",
        "# Check class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='dx', data=metadata)\n",
        "plt.title('Distribution of Skin Lesion Classes')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dePzxwCYQsBH"
      },
      "outputs": [],
      "source": [
        "class_counts = metadata['dx'].value_counts()\n",
        "print(\"Class distribution:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"{class_name}: {count} images ({count/len(metadata)*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M58rRdEUD_P"
      },
      "source": [
        "### 3.3.2 Loading the dataset\n",
        "\n",
        "Now, let's create a custom PyTorch dataset for loading the HAM10000 images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgSRF0dVQsDH"
      },
      "outputs": [],
      "source": [
        "class SkinLesionDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Dataframe with image metadata\n",
        "            image_dir (string): Directory with all the images\n",
        "            transform (callable, optional): Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create a mapping from diagnosis to integer label\n",
        "        self.classes = sorted(df['dx'].unique())\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get image ID and path\n",
        "        img_id = self.df.iloc[idx]['image_id']\n",
        "        # Check both folders since images are split between them\n",
        "        img_path = os.path.join(self.image_dir, 'HAM10000_images_part_1', f\"{img_id}.jpg\")\n",
        "        if not os.path.exists(img_path):\n",
        "            img_path = os.path.join(self.image_dir, 'HAM10000_images_part_2', f\"{img_id}.jpg\")\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Get label\n",
        "        diagnosis = self.df.iloc[idx]['dx']\n",
        "        label = self.class_to_idx[diagnosis]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3.3 Adding transformations\n",
        "\n",
        "To make our model more robust, and to augment the dataset, we can apply some transformations to the images. These transformations include resizing, random cropping, and normalization. We will also convert the images to PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdqRcxq_QsFb"
      },
      "outputs": [],
      "source": [
        "# Define data transformations\n",
        "# Data augmentation is done only for training dataset\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Normalizing pixels based on ImageNet's average RGB values\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3.4 Splitting the dataset and creating data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPPTKc1rQsHe"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation sets\n",
        "train_df, val_df = train_test_split(metadata, test_size=0.2, random_state=42, stratify=metadata['dx'])\n",
        "\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SkinLesionDataset(\n",
        "    df=train_df,\n",
        "    image_dir='.',  # Adjust this path as needed\n",
        "    transform=data_transforms['train']\n",
        ")\n",
        "\n",
        "val_dataset = SkinLesionDataset(\n",
        "    df=val_df,\n",
        "    image_dir='.',  # Adjust this path as needed\n",
        "    transform=data_transforms['val']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY5NgeqeQi7Y"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Get class names for reference\n",
        "class_names = train_dataset.classes\n",
        "print(f\"Class names: {class_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXuRoZDuUSUw"
      },
      "source": [
        "## 3.4 Training a CNN Model\n",
        "\n",
        "### 3.4.1 Defining the model architecture\n",
        "\n",
        "Now let's build our CNN architecture. We'll use a simple architecture with a few convolutional layers followed by fully connected layers. The model will take an input image of size 224x224 and output the probabilities for each of the 7 classes. Note the use of batch normalization, pooling, and dropout layers to improve the model's performance and prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sxCKvLhUMo9"
      },
      "outputs": [],
      "source": [
        "class SkinLesionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(SkinLesionCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # After 4 max-pooling operations with 224x224 input: 224/(2^4) = 14\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional block\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "\n",
        "        # Second convolutional block\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        # Third convolutional block\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        # Fourth convolutional block\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(-1, 256 * 14 * 14)\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zktcNPKcUMq3"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = SkinLesionCNN(num_classes=len(class_names))\n",
        "model = model.to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4.2 Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LsX56o6UMvK"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Train the model and evaluate on validation set after each epoch\n",
        "    \"\"\"\n",
        "    # Track best model\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = model.state_dict()\n",
        "\n",
        "    # Track loss and accuracy\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass + optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc.item())\n",
        "\n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # No gradient during validation\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accs.append(epoch_acc.item())\n",
        "\n",
        "        print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Save the best model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, val_losses, train_accs, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96kY49g0Ubjo"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 15\n",
        "model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer, num_epochs\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'skin_lesion_cnn.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlRVrpkWUj04"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on the given dataloader\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beUoW-PTUj2Z"
      },
      "outputs": [],
      "source": [
        "y_true, y_pred = evaluate_model(model, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnNWFivvUj3h"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF9Ip6LAUbn7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH2v9Bz_Uvju"
      },
      "source": [
        "### 3.4.3 Visualizing the results\n",
        "\n",
        "Let's visualize the training process and some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2BtS-ZaUqoZ"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ6qb14-Uqqr"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accs, 'b-', label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), val_accs, 'r-', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6ueDY18Uzwg"
      },
      "outputs": [],
      "source": [
        "# Function to visualize predictions\n",
        "def visualize_predictions(model, dataloader, class_names, num_images=12):\n",
        "    \"\"\"\n",
        "    Visualize some predictions from the model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    _ = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(3, 4, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'True: {class_names[labels[j]]}\\nPred: {class_names[preds[j]]}',\n",
        "                            color=('green' if preds[j] == labels[j] else 'red'))\n",
        "\n",
        "                # Denormalize image\n",
        "                img = inputs[j].cpu().numpy().transpose((1, 2, 0))\n",
        "                mean = np.array([0.485, 0.456, 0.406])\n",
        "                std = np.array([0.229, 0.224, 0.225])\n",
        "                img = std * img + mean\n",
        "                img = np.clip(img, 0, 1)\n",
        "\n",
        "                plt.imshow(img)\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                    return\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4UMlbKDU2vN"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "visualize_predictions(model, val_loader, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOS-QnqmU6P0"
      },
      "source": [
        "## 3.5 Making predictions\n",
        "\n",
        "Let's create a function to make predictions on new images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkrwaD_lU2xV"
      },
      "outputs": [],
      "source": [
        "def predict_image(model, image_path, transform, class_names):\n",
        "    \"\"\"\n",
        "    Make a prediction on a single image\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        confidence, prediction = torch.max(probabilities, 1)\n",
        "\n",
        "    # Get prediction and confidence\n",
        "    predicted_class = class_names[prediction.item()]\n",
        "    confidence_score = confidence.item()\n",
        "\n",
        "    # Display image and prediction\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f'Prediction: {predicted_class}\\nConfidence: {confidence_score:.4f}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Return all class probabilities\n",
        "    probs = probabilities.cpu().numpy()[0]\n",
        "    for i, (class_name, prob) in enumerate(zip(class_names, probs)):\n",
        "        print(f\"{class_name}: {prob:.4f}\")\n",
        "\n",
        "    return predicted_class, confidence_score\n",
        "\n",
        "# Example usage (replace with your image path)\n",
        "# predict_image(model, 'path_to_your_image.jpg', data_transforms['val'], class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.6 Simplifying with Pytorch lightning \n",
        "\n",
        "As we have seen in the previous chapters, PyTorch Lightning is a lightweight wrapper around PyTorch that helps to organize PyTorch code. It can as easily be used for CNNs as for other model types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "# Define a PyTorch Lightning module for the skin lesion CNN\n",
        "class LitSkinLesionCNN(pl.LightningModule):\n",
        "    def __init__(self, num_classes=7, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()  # Saves hyperparameters for logging and checkpointing\n",
        "        self.lr = lr\n",
        "\n",
        "        # Define the CNN architecture (same as before)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the CNN\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Training step: computes loss and logs accuracy\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
        "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Validation step: computes loss and logs accuracy\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
        "        self.log('val_acc', acc, on_step=False, on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Define optimizer for training\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "# Example usage:\n",
        "# model = LitSkinLesionCNN(num_classes=len(class_names))\n",
        "# trainer = pl.Trainer(max_epochs=15, accelerator=\"auto\")\n",
        "# trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.7 Optional exercise: Transfer learning \n",
        "\n",
        "Load the pretrained [resnet50](https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/) model and finetune it on the skin lesion dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNetTransferModel(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(ResNetTransferModel, self).__init__()\n",
        "        # Load pre-trained ResNet50\n",
        "        self.resnet = ...\n",
        "\n",
        "        # Freeze the early layers\n",
        "\n",
        "        # Replace the final fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initiailize the transfer learning model\n",
        "transfer_model = ResNetTransferModel(num_classes=len(class_names))\n",
        "transfer_model = transfer_model.to(device)\n",
        "\n",
        "# Define loss function and optimizer for the transfer learning model\n",
        "transfer_criterion = nn.CrossEntropyLoss()\n",
        "transfer_optimizer = optim.Adam(transfer_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Train the transfer learning model\n",
        "num_epochs_transfer = 10\n",
        "transfer_model, tl_train_losses, tl_val_losses, tl_train_accs, tl_val_accs = train_model(\n",
        "    transfer_model, train_loader, val_loader, transfer_criterion, transfer_optimizer, num_epochs_transfer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained transfer learning model\n",
        "torch.save(transfer_model.state_dict(), 'skin_lesion_transfer_learning.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the transfer learning model\n",
        "y_true_tl, y_pred_tl = evaluate_model(transfer_model, val_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report (Transfer Learning):\")\n",
        "print(classification_report(y_true_tl, y_pred_tl, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm_tl = confusion_matrix(y_true_tl, y_pred_tl)\n",
        "sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Transfer Learning)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare the two models\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), val_accs, 'b-', label='Custom CNN')\n",
        "plt.plot(range(1, num_epochs_transfer+1), tl_val_accs, 'r-', label='Transfer Learning')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), val_losses, 'b-', label='Custom CNN')\n",
        "plt.plot(range(1, num_epochs_transfer+1), tl_val_losses, 'r-', label='Transfer Learning')\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions from the transfer learning model\n",
        "visualize_predictions(transfer_model, val_loader, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yhBxuduU5p8"
      },
      "outputs": [],
      "source": [
        "class ResNetTransferModel(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(ResNetTransferModel, self).__init__()\n",
        "        # Load pre-trained ResNet50\n",
        "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze the early layers\n",
        "        for param in list(self.resnet.parameters())[:-20]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the final fully connected layer\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Initialize the transfer learning model\n",
        "transfer_model = ResNetTransferModel(num_classes=len(class_names))\n",
        "transfer_model = transfer_model.to(device)\n",
        "\n",
        "# Define loss function and optimizer for the transfer learning model\n",
        "transfer_criterion = nn.CrossEntropyLoss()\n",
        "transfer_optimizer = optim.Adam(transfer_model.parameters(), lr=0.0001)\n",
        "\n",
        "# Train the transfer learning model\n",
        "num_epochs_transfer = 10\n",
        "transfer_model, tl_train_losses, tl_val_losses, tl_train_accs, tl_val_accs = train_model(\n",
        "    transfer_model, train_loader, val_loader, transfer_criterion, transfer_optimizer, num_epochs_transfer\n",
        ")\n",
        "\n",
        "# Save the trained transfer learning model\n",
        "torch.save(transfer_model.state_dict(), 'skin_lesion_transfer_learning.pth')\n",
        "\n",
        "# Evaluate the transfer learning model\n",
        "y_true_tl, y_pred_tl = evaluate_model(transfer_model, val_loader)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report (Transfer Learning):\")\n",
        "print(classification_report(y_true_tl, y_pred_tl, target_names=class_names))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm_tl = confusion_matrix(y_true_tl, y_pred_tl)\n",
        "sns.heatmap(cm_tl, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Transfer Learning)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare the two models\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), val_accs, 'b-', label='Custom CNN')\n",
        "plt.plot(range(1, num_epochs_transfer+1), tl_val_accs, 'r-', label='Transfer Learning')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), val_losses, 'b-', label='Custom CNN')\n",
        "plt.plot(range(1, num_epochs_transfer+1), tl_val_losses, 'r-', label='Transfer Learning')\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions from the transfer learning model\n",
        "visualize_predictions(transfer_model, val_loader, class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
