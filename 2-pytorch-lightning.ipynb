{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ab24db",
   "metadata": {},
   "source": [
    "# 2 Working faster and cleaner with PyTorch Lightning\n",
    "\n",
    "## 2.1 Introduction\n",
    "\n",
    "In the previous tutorial, we learned how to build a simple neural network using PyTorch. In this tutorial, we will learn how to use PyTorch Lightning to make our code cleaner and more efficient.\n",
    "\n",
    "PyTorch Lightning is a lightweight wrapper around PyTorch that helps you organize your code and decouple the science code from the engineering code. It provides a high-level interface for training and testing models, making it easier to write clean and maintainable code. It also helps you to scale your code to multiple GPUs and TPUs, and to run it on different platforms (e.g., cloud, local, etc.) without changing your code. \n",
    "\n",
    "We will also briefly look at Weights & Biases (wandb), a tool for tracking experiments and visualizing results. It is not required to use PyTorch Lightning, but it is a great tool to have in your toolbox. We will use it to log our training and validation metrics, and to visualize our results.\n",
    "\n",
    "## 2.2 PyTorch Lightning\n",
    "\n",
    "### 2.2.1 Repeating the basics\n",
    "\n",
    "First, lets get some code from the previous tutorial. We will use the same dataset and model, but we will refactor the code to use PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaaffd-9105-48d6-862d-6c06e8b4551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (uncomment if needed)\n",
    "# !pip install lightning wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d4950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, features: pd.DataFrame, targets: pd.Series):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class BreastCancerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(30, 20)\n",
    "        self.layer2 = nn.Linear(20, 10)\n",
    "        self.layer3 = nn.Linear(10, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "cancer_data = load_breast_cancer(as_frame=True)\n",
    "cancer_dataset = CancerDataset(cancer_data['data'], cancer_data['target'])\n",
    "\n",
    "data_train, data_validation = torch.utils.data.random_split(cancer_dataset, [0.8, 0.2])\n",
    "dataloader_train = DataLoader(data_train, batch_size=32, shuffle=True)\n",
    "dataloader_validation = DataLoader(data_validation, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e34424",
   "metadata": {},
   "source": [
    "### 2.2.2 Setting up the Lightning module\n",
    "\n",
    "Take a look again at the training loop code from the previous tutorial. It contained quite some boilerplate code that would be highly similar across different ML projects. We will now refactor this code to use PyTorch Lightning, which will help us to remove a lot of this boilerplate code.\n",
    "\n",
    "To do this, we will create a new class that inherits from [`LightningModule`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#lightningmodule). This class will contain all the code for training and validating our model. Similar to the `nn.Module` class, we will need to implement the `__init__` method to initialize our model and the `forward` method to define the forward pass. However, we will also need to implement some additional methods that are specific to PyTorch Lightning:\n",
    "\n",
    "- `training_step`: This method will be called for each batch of training data. It will contain the code for the forward pass and the loss calculation.\n",
    "- `validation_step`: This method will be called for each batch of validation data. It will contain the code for the forward pass and the loss calculation.\n",
    "- `configure_optimizers`: This method will be called to configure the optimizer and the learning rate scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "class BreastCancerModule(L.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BreastCancerClassifier()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f4371",
   "metadata": {},
   "source": [
    "Note that the resulting code is much cleaner and more organized than the training loop code we had before.\n",
    "\n",
    "### 2.2.3 Setting up the Lightning trainer\n",
    "\n",
    "All configuration of the training process is done in the [`Trainer`](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer) class. This class will take care of the training and validation loops, as well as logging and checkpointing. We will create an instance of this class and pass it our model and the training and validation data loaders.\n",
    "\n",
    "*Assignment: Browse through the documentation for the trainer class and try to understand the different arguments of the class. Pay special attention to the `accelerator` and `devices` arguments, which are some of the most useful features of PyTorch Lightning.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fec5f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/ralfgabriels/.local/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /shared/software/miniconda/envs/python-3.12/lib/pyth ...\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/shared/home/ralfgabriels/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",  # Automatically selects GPU or CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4b4b0",
   "metadata": {},
   "source": [
    "Note that the trainer immediately tells us whether we are using a GPU or not. This is a great feature of PyTorch Lightning, as it allows us to write code that is agnostic to the hardware we are using. We can run the same code on a CPU, a single GPU, or multiple GPUs without changing anything in our code.\n",
    "\n",
    "### 2.2.4 Fitting the model\n",
    "\n",
    "Now we can simply call the `fit` method of the trainer to start training our model. It takes the model, the training data loader, and the validation data loader as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    BreastCancerModule(learning_rate=0.001),\n",
    "    train_dataloaders=dataloader_train,\n",
    "    val_dataloaders=dataloader_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c143cb",
   "metadata": {},
   "source": [
    "While we had to take care of logging the progress ourselves in the previous tutorial, PyTorch Lightning takes care of this for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2d9a8",
   "metadata": {},
   "source": [
    "## 2.3 Logging with Weights & Biases\n",
    "\n",
    "Weights & Biases (wandb) allows us to log and visualize training and validation metrics across different training runs. It also has a feature for hyperparameter tuning, called Sweeps, which allows us to automatically search for the best hyperparameters for our model.\n",
    "\n",
    "To use wandb, first go to the [wandb website](https://wandb.ai/) and create an account. You can easily sign in with an existing GitHub, Google, or Microsoft account. After signing in, you will be taken to the dashboard, where you can create a new project. You can also create a new API key, which you will need to use wandb in your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab788b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"breast-cancer-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b15b69",
   "metadata": {},
   "source": [
    "Now we can add the wandb logger to our PyTorch Lightning trainer with the `logger`argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    logger=L.pytorch.loggers.WandbLogger(\n",
    "        project=\"breast-cancer-classification\",\n",
    "        log_model=True,\n",
    "    ))\n",
    "\n",
    "trainer.fit(\n",
    "    BreastCancerModule(learning_rate=0.001),\n",
    "    train_dataloaders=dataloader_train,\n",
    "    val_dataloaders=dataloader_validation,\n",
    ")\n",
    "\n",
    "wandb.finish()  # Finish the wandb run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3803a4",
   "metadata": {},
   "source": [
    "Go to the run URL as logged by wandb and check out the results. You should see a new run with the same name as the one you used in the code. You can click on it to see the details of the run, including the training and validation metrics, the model checkpoints, and the hyperparameters used for the run.\n",
    "\n",
    "## 2.4 Performing a hyperparameter sweep\n",
    "\n",
    "Now that we have set up wandb, we can use it to perform a hyperparameter sweep. This will allow us to automatically search for the best hyperparameters for our model, such as number of layers, number of neurons per layer, learning rate, etc.\n",
    "\n",
    "To do this, we must first update the module to accept hyperparameters as arguments. To make the number of layers and the number of neurons per layer configurable, we will implement a loop in the `__init__` method that creates the layers based on the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerClassifier(nn.Module):\n",
    "    def __init__(self, hidden_layers=1, hidden_neurons=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(30, hidden_neurons))\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_neurons, hidden_neurons))\n",
    "        self.layers.append(nn.Linear(hidden_neurons, 1))\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Iterate over all layers except the last one and add activations\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        # Last layer without activation\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254625f",
   "metadata": {},
   "source": [
    "We must also slightly modify the Lightning module to accept the hyperparameters as arguments and pass them on to the model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a53459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerModule(L.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001, hidden_layers=1, hidden_neurons=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BreastCancerClassifier(hidden_layers, hidden_neurons)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee171bea",
   "metadata": {},
   "source": [
    "The following function will take the hyperparameters as arguments and setup the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(*args, **kwargs):\n",
    "    # Create the trainer\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"auto\",\n",
    "        logger=L.pytorch.loggers.WandbLogger(\n",
    "            project=\"breast-cancer-classification\",\n",
    "            log_model=True,\n",
    "        ))\n",
    "\n",
    "    trainer.fit(\n",
    "        BreastCancerModule(*args, **kwargs),\n",
    "        train_dataloaders=dataloader_train,\n",
    "        val_dataloaders=dataloader_validation,\n",
    "    )\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b9c72",
   "metadata": {},
   "source": [
    "Now, we can create a sweep configuration. This configuration will define the hyperparameters we want to search over and the values we want to try. We will use the `wandb` library to create a sweep configuration. To configure sweeps, check out the [wandb documentation](https://docs.wandb.ai/guides/sweeps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"hidden_layers\": {\n",
    "            \"values\": [0, 1, 2]\n",
    "        },\n",
    "        \"hidden_neurons\": {\n",
    "            \"values\": [2, 4, 8, 16, 32]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.01\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f788fbd",
   "metadata": {},
   "source": [
    "Here, we initialize the sweep with its configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6985498",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"breast-cancer-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc0b45",
   "metadata": {},
   "source": [
    "With the `wandb.agent` function, we can start the sweep. Note that the `count` argument defines how many runs we want to perform. If it would not be set, the sweep would run indefinitely and keep testing different hyperparameters until we stop it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=sweep, count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09456043",
   "metadata": {},
   "source": [
    "*Assignment: Go to the wandb dashboard and check out the results of the sweep. You should see a new sweep with the same name as the one you used in the code. You can click on it to see the details of the sweep, including the training and validation metrics, the model checkpoints, and the hyperparameters used for each run.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
