{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02455ac",
   "metadata": {},
   "source": [
    "\n",
    "# RNN Tutorial for DNA Sequence Classification\n",
    "\n",
    "In this tutorial, we'll build a Recurrent Neural Network (RNN) to classify DNA sequences using PyTorch. This is a common bioinformatics task that's well-suited for sequence models like RNNs.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to RNNs](#introduction-to-rnns)\n",
    "2. [Setting Up the Environment](#setting-up-the-environment)\n",
    "3. [Preparing DNA Sequence Data](#preparing-dna-sequence-data)\n",
    "4. [Building the RNN Model](#building-the-rnn-model)\n",
    "5. [Training the Model](#training-the-model)\n",
    "6. [Evaluating and Testing](#evaluating-and-testing)\n",
    "7. [Advanced Techniques](#advanced-techniques)\n",
    "8. [Conclusion](#conclusion)\n",
    "\n",
    "## Introduction to RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to work with sequential data. Unlike feedforward networks, RNNs have connections that form cycles, allowing information to persist across processing steps. This makes them particularly useful for tasks like:\n",
    "\n",
    "- Natural language processing\n",
    "- Time series prediction\n",
    "- **Biological sequence analysis (our focus today)**\n",
    "\n",
    "RNNs process sequences one element at a time while maintaining an internal \"memory\" of what they've seen before. This is perfect for DNA sequences, which are naturally sequential data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ceeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5687e77",
   "metadata": {},
   "source": [
    "## Preparing DNA Sequence Data\n",
    "\n",
    "### Step 1: Load and explore your dataset\n",
    "\n",
    "Let's assume you have a dataset of DNA sequences and their corresponding labels. We'll create a PyTorch dataset class to handle this data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNASequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "        # Define nucleotide mapping\n",
    "        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert sequence to numerical representation\n",
    "        sequence_encoded = [self.nucleotide_to_idx[nucleotide] for nucleotide in sequence]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sequence_tensor = torch.tensor(sequence_encoded, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sequence_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d3a87",
   "metadata": {},
   "source": [
    "### Step 2: Load your dataset and split into train/validation/test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a09b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data loading (replace with your actual data loading code)\n",
    "def load_dna_data():\n",
    "\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/master/practicum/Classification/acceptor_sites_dataset_train.csv\")\n",
    "    sequences = df['sequence'].values\n",
    "    labels = df['label'].values\n",
    "    labels = [0 if label == -1 else 1 for label in labels]\n",
    "    return sequences, labels\n",
    "\n",
    "# Load data\n",
    "sequences, labels = load_dna_data()\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "seqs_train, seqs_temp, labels_train, labels_temp = train_test_split(\n",
    "    sequences, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "seqs_val, seqs_test, labels_val, labels_test = train_test_split(\n",
    "    seqs_temp, labels_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(seqs_train)}\")\n",
    "print(f\"Validation samples: {len(seqs_val)}\")\n",
    "print(f\"Testing samples: {len(seqs_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44dfc7",
   "metadata": {},
   "source": [
    "### Step 3: Create data loaders\n",
    "\n",
    "Question: How do we deal with different sequence lengths in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pad sequences to the same length for batch processing\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # Find the length of the longest sequence\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded_seq = torch.zeros(max_length, dtype=torch.long)\n",
    "        padded_seq[:len(seq)] = seq\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    # Stack sequences and labels into batches\n",
    "    sequences_batch = torch.stack(padded_sequences)\n",
    "    labels_batch = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return sequences_batch, labels_batch\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DNASequenceDataset(seqs_train, labels_train)\n",
    "val_dataset = DNASequenceDataset(seqs_val, labels_val)\n",
    "test_dataset = DNASequenceDataset(seqs_test, labels_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac89db",
   "metadata": {},
   "source": [
    "## Building the RNN Model\n",
    "\n",
    "Now, let's define our RNN model for DNA sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNASequenceRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers=1, dropout=0.2):\n",
    "        super(DNASequenceRNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer to convert nucleotide indices to dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # RNN layer (can be vanilla RNN, LSTM, or GRU)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "\n",
    "        # Embed nucleotides\n",
    "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Pass through RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "        # We only need the output from the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # (batch_size, output_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c40f8",
   "metadata": {},
   "source": [
    "### Step 4: Initialize model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27201929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = 4  # Number of nucleotides (A, C, G, T)\n",
    "embedding_dim = 8  # Size of embedding vectors\n",
    "hidden_dim = 64  # Size of hidden layer\n",
    "output_size = 2  # Number of classes (binary classification)\n",
    "num_layers = 2  # Number of RNN layers\n",
    "dropout = 0.2  # Dropout rate\n",
    "\n",
    "# Create model\n",
    "model = DNASequenceRNN(input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e570c48",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff78704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train_losses, val_losses, val_accuracies = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d178c",
   "metadata": {},
   "source": [
    "## Evaluating and Testing\n",
    "\n",
    "After training, let's evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "\n",
    "    return test_loss, accuracy, report\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy, test_report = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f3b92",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Making predictions with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert sequence to indices\n",
    "    nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    sequence_encoded = [nucleotide_to_idx[nucleotide] for nucleotide in sequence]\n",
    "    sequence_tensor = torch.tensor(sequence_encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        # Get probability scores\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "\n",
    "    return predicted.item(), probabilities.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "example_sequence = \"ATCGATCGATCGATCG\"\n",
    "predicted_class, probabilities = predict_sequence(model, example_sequence, device)\n",
    "\n",
    "print(f\"Sequence: {example_sequence}\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb448849",
   "metadata": {},
   "source": [
    "## Bonus: Using Attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDNASequenceRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers=1, dropout=0.2):\n",
    "        super(AttentionDNASequenceRNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed nucleotides\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Pass through RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_scores = self.attention(out).squeeze(-1)  # (batch_size, seq_len)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Apply attention weights\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "        weighted_output = out * attention_weights  # (batch_size, seq_len, hidden_dim)\n",
    "        context_vector = weighted_output.sum(dim=1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(context_vector)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
