{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hSaal3GT5de"
      },
      "source": [
        "\n",
        "# RNN Tutorial for DNA Sequence Classification\n",
        "\n",
        "In this tutorial, we'll build a Recurrent Neural Network (RNN) to classify DNA sequences using PyTorch. This is a common bioinformatics task that's well-suited for sequence models like RNNs.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to RNNs](#introduction-to-rnns)\n",
        "2. [Setting Up the Environment](#setting-up-the-environment)\n",
        "3. [Preparing DNA Sequence Data](#preparing-dna-sequence-data)\n",
        "4. [Building the RNN Model](#building-the-rnn-model)\n",
        "5. [Training the Model](#training-the-model)\n",
        "6. [Evaluating and Testing](#evaluating-and-testing)\n",
        "7. [Advanced Techniques](#advanced-techniques)\n",
        "8. [Conclusion](#conclusion)\n",
        "\n",
        "## Introduction to RNNs\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks designed to work with sequential data. Unlike feedforward networks, RNNs have connections that form cycles, allowing information to persist across processing steps. This makes them particularly useful for tasks like:\n",
        "\n",
        "- Natural language processing\n",
        "- Time series prediction\n",
        "- **Biological sequence analysis (our focus today)**\n",
        "\n",
        "RNNs process sequences one element at a time while maintaining an internal \"memory\" of what they've seen before. This is perfect for DNA sequences, which are naturally sequential data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGBAGy9lJ7x_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZLKVb7UCAi"
      },
      "source": [
        "## Preparing DNA Sequence Data\n",
        "\n",
        "### Step 1: Load and explore your dataset\n",
        "\n",
        "Let's assume you have a dataset of DNA sequences and their corresponding labels. We'll create a PyTorch dataset class to handle this data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DNASequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for DNA sequences and their corresponding labels.\n",
        "\n",
        "    This dataset converts DNA sequences into numerical representations\n",
        "    based on a predefined nucleotide-to-index mapping and provides\n",
        "    PyTorch tensors for sequences and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sequences, labels):\n",
        "        \"\"\"\n",
        "        Initializes the DNASequenceDataset.\n",
        "\n",
        "        Args:\n",
        "            sequences (list of str): A list of DNA sequences, where each sequence is a string of nucleotides ('A', 'C', 'G', 'T').\n",
        "            labels (list of int): A list of integer labels corresponding to each DNA sequence.\n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "        # Define nucleotide mapping\n",
        "        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of DNA sequences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the sequence and label at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - sequence_tensor (torch.Tensor): A tensor of the numerical representation of the DNA sequence.\n",
        "                - label_tensor (torch.Tensor): A tensor of the corresponding label.\n",
        "        \"\"\"\n",
        "        sequence = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert sequence to numerical representation\n",
        "        sequence_encoded = [self.nucleotide_to_idx[nucleotide] for nucleotide in sequence]\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        sequence_tensor = torch.tensor(sequence_encoded, dtype=torch.long)\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return sequence_tensor, label_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1IcUHK3ULU8"
      },
      "source": [
        "### Step 2: Load your dataset and split into train/validation/test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dna_data():\n",
        "    \"\"\"\n",
        "    Loads DNA sequence data from a CSV file and processes it.\n",
        "\n",
        "    The function reads a dataset containing DNA sequences and their labels,\n",
        "    converts the labels from -1/1 to 0/1, and returns the sequences and labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - sequences (numpy.ndarray): An array of DNA sequences as strings.\n",
        "            - labels (list of int): A list of integer labels (0 or 1) corresponding to each sequence.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/master/practicum/Classification/acceptor_sites_dataset_train.csv\")\n",
        "    sequences = df['sequence'].values\n",
        "    labels = df['label'].values\n",
        "    labels = [0 if label == -1 else 1 for label in labels]\n",
        "    return sequences, labels\n",
        "\n",
        "\n",
        "# Load data\n",
        "sequences, labels = load_dna_data()\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "seqs_train, seqs_temp, labels_train, labels_temp = train_test_split(\n",
        "    sequences, labels, test_size=0.3, random_state=42\n",
        ")\n",
        "seqs_val, seqs_test, labels_val, labels_test = train_test_split(\n",
        "    seqs_temp, labels_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(seqs_train)}\")\n",
        "print(f\"Validation samples: {len(seqs_val)}\")\n",
        "print(f\"Testing samples: {len(seqs_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8caDj6aUPUO"
      },
      "source": [
        "### Step 3: Create data loaders\n",
        "\n",
        "Question: How do we deal with different sequence lengths in the dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kU8CYMSRCnm"
      },
      "outputs": [],
      "source": [
        "# We need to pad sequences to the same length for batch processing\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to prepare batches of DNA sequences and labels.\n",
        "\n",
        "    This function pads all DNA sequences in a batch to the same length (the length\n",
        "    of the longest sequence in the batch) and stacks them into a single tensor.\n",
        "    It also converts the labels into a tensor.\n",
        "\n",
        "    Args:\n",
        "        batch (list of tuples): A batch of data where each element is a tuple\n",
        "                                containing a DNA sequence (torch.Tensor) and its label (int).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - sequences_batch (torch.Tensor): A tensor of padded DNA sequences with shape (batch_size, max_length).\n",
        "            - labels_batch (torch.Tensor): A tensor of labels with shape (batch_size,).\n",
        "    \"\"\"\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # Find the length of the longest sequence\n",
        "    max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        padded_seq = torch.zeros(max_length, dtype=torch.long)\n",
        "        padded_seq[:len(seq)] = seq\n",
        "        padded_sequences.append(padded_seq)\n",
        "\n",
        "    # Stack sequences and labels into batches\n",
        "    sequences_batch = torch.stack(padded_sequences)\n",
        "    labels_batch = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return sequences_batch, labels_batch\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DNASequenceDataset(seqs_train, labels_train)\n",
        "val_dataset = DNASequenceDataset(seqs_val, labels_val)\n",
        "test_dataset = DNASequenceDataset(seqs_test, labels_test)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faXV2ljsUSwo"
      },
      "source": [
        "## Building the RNN Model\n",
        "\n",
        "Now, let's define our RNN model for DNA sequence classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j06LevdRRRHG"
      },
      "outputs": [],
      "source": [
        "class DNASequenceRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A Recurrent Neural Network (RNN) model for classifying DNA sequences.\n",
        "\n",
        "    This model uses an embedding layer to convert nucleotide indices into dense vectors,\n",
        "    followed by an RNN layer (vanilla RNN, LSTM, or GRU) to process sequential data,\n",
        "    and a fully connected layer for classification.\n",
        "\n",
        "    Attributes:\n",
        "        embedding (nn.Embedding): Embedding layer to convert nucleotide indices to dense vectors.\n",
        "        rnn (nn.RNN): Recurrent Neural Network layer for sequence processing.\n",
        "        fc (nn.Linear): Fully connected layer for output predictions.\n",
        "        dropout (nn.Dropout): Dropout layer for regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers=1, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Initializes the DNASequenceRNN model.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): The size of the input vocabulary (number of unique nucleotides).\n",
        "            embedding_dim (int): The dimension of the embedding vectors.\n",
        "            hidden_dim (int): The number of features in the hidden state of the RNN.\n",
        "            output_size (int): The number of output classes.\n",
        "            num_layers (int, optional): The number of RNN layers. Defaults to 1.\n",
        "            dropout (float, optional): Dropout probability for regularization. Defaults to 0.2.\n",
        "        \"\"\"\n",
        "        super(DNASequenceRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer to convert nucleotide indices to dense vectors\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "\n",
        "        # RNN layer (can be vanilla RNN, LSTM, or GRU)\n",
        "        # Activity: Try replacing nn.RNN with nn.LSTM or nn.GRU for experimentation\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_dim,  # Input size to RNN is the embedding dimension\n",
        "            hidden_size=hidden_dim,    # Number of features in the hidden state\n",
        "            num_layers=num_layers,     # Number of stacked RNN layers\n",
        "            batch_first=True,          # Input and output tensors are (batch, seq, feature)\n",
        "            dropout=dropout if num_layers > 1 else 0  # Apply dropout only if num_layers > 1\n",
        "        )\n",
        "\n",
        "        # Output layer to map hidden state to output classes\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the DNASequenceRNN model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length),\n",
        "                              where each value is an index representing a nucleotide.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, output_size),\n",
        "                          containing the predicted class scores for each sequence.\n",
        "        \"\"\"\n",
        "        # x shape: (batch_size, sequence_length)\n",
        "\n",
        "        # Embed nucleotides into dense vectors\n",
        "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Pass through RNN\n",
        "        out, _ = self.rnn(x, h0)  # out: (batch_size, sequence_length, hidden_dim)\n",
        "\n",
        "        # We only need the output from the last time step\n",
        "        out = out[:, -1, :]  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Pass through the fully connected layer to get predictions\n",
        "        out = self.fc(out)  # (batch_size, output_size)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_OZNu-fUV2F"
      },
      "source": [
        "### Step 4: Initialize model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o9_RNR-RRLL"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "input_size = 4  # Number of nucleotides (A, C, G, T)\n",
        "embedding_dim = 8  # Size of embedding vectors\n",
        "hidden_dim = 64  # Size of hidden layer\n",
        "output_size = 2  # Number of classes (binary classification)\n",
        "num_layers = 2  # Number of RNN layers\n",
        "dropout = 0.2  # Dropout rate\n",
        "\n",
        "# Create model\n",
        "model = DNASequenceRNN(input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M9dHN8iUYwl"
      },
      "source": [
        "## Training the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDxnVR4ERTW9"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Trains a PyTorch model using the provided training and validation data loaders.\n",
        "\n",
        "    This function performs the training and validation loop for a specified number of epochs,\n",
        "    calculates training and validation losses, and tracks validation accuracy.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be trained.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        criterion (nn.Module): Loss function to optimize.\n",
        "        optimizer (torch.optim.Optimizer): Optimization algorithm.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "        device (torch.device): Device to run the training on (e.g., 'cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - train_losses (list of float): List of average training losses for each epoch.\n",
        "            - val_losses (list of float): List of average validation losses for each epoch.\n",
        "            - val_accuracies (list of float): List of validation accuracies for each epoch.\n",
        "    \"\"\"\n",
        "    # Lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for sequences, labels in train_loader:\n",
        "            # Move data to the specified device\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate training loss\n",
        "            train_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "        # Calculate average training loss\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for validation\n",
        "            for sequences, labels in val_loader:\n",
        "                # Move data to the specified device\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Accumulate validation loss\n",
        "                val_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average validation loss and accuracy\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Print metrics for the current epoch\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {train_loss:.4f}')\n",
        "        print(f'  Val Loss: {val_loss:.4f}')\n",
        "        print(f'  Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 50\n",
        "train_losses, val_losses, val_accuracies = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWub8cmeUeL2"
      },
      "source": [
        "## Evaluating and Testing\n",
        "\n",
        "After training, let's evaluate our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ekU8HY0RYvE"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a PyTorch model on a test dataset.\n",
        "\n",
        "    This function computes the test loss, accuracy, and generates a classification report\n",
        "    using the provided test data loader. It does not perform gradient updates.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "        criterion (nn.Module): Loss function to compute the test loss.\n",
        "        device (torch.device): Device to run the evaluation on (e.g., 'cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - test_loss (float): The average test loss.\n",
        "            - accuracy (float): The accuracy of the model on the test dataset.\n",
        "            - report (str): A classification report with precision, recall, and F1-score.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    predictions = []  # List to store predicted labels\n",
        "    true_labels = []  # List to store true labels\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for sequences, labels in test_loader:\n",
        "            # Move data to the specified device\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass to get model predictions\n",
        "            outputs = model(sequences)\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "\n",
        "            # Accumulate the total test loss\n",
        "            test_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "            # Get the predicted class labels\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Store predictions and true labels for metrics calculation\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate the average test loss\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "    # Calculate accuracy and generate a classification report\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions)\n",
        "\n",
        "    return test_loss, accuracy, report\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy, test_report = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj_JgncBUg_s"
      },
      "source": [
        "\n",
        "### Step 5: Making predictions with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prhHtwk6RY1h"
      },
      "outputs": [],
      "source": [
        "def predict_sequence(model, sequence, device):\n",
        "    \"\"\"\n",
        "    Predicts the class of a given DNA sequence using a trained model.\n",
        "\n",
        "    This function encodes a DNA sequence into numerical indices, processes it through\n",
        "    the model, and returns the predicted class along with the probability scores for\n",
        "    each class.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained PyTorch model for sequence classification.\n",
        "        sequence (str): The DNA sequence to classify (e.g., \"ATCG\").\n",
        "        device (torch.device): The device to run the prediction on (e.g., 'cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - predicted_class (int): The predicted class index.\n",
        "            - probabilities (numpy.ndarray): An array of probability scores for each class.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Map nucleotides to indices\n",
        "    nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "    # Encode the DNA sequence into numerical indices\n",
        "    sequence_encoded = [nucleotide_to_idx[nucleotide] for nucleotide in sequence]\n",
        "\n",
        "    # Convert the encoded sequence into a tensor and add a batch dimension\n",
        "    sequence_tensor = torch.tensor(sequence_encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for prediction\n",
        "        # Forward pass through the model\n",
        "        output = model(sequence_tensor)\n",
        "\n",
        "        # Get the predicted class (index with the highest score)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "        # Compute probability scores using softmax\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n",
        "\n",
        "    return predicted.item(), probabilities.cpu().numpy()\n",
        "\n",
        "# Example usage\n",
        "example_sequence = \"ATCGATCGATCGATCG\"  # Input DNA sequence\n",
        "predicted_class, probabilities = predict_sequence(model, example_sequence, device)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Sequence: {example_sequence}\")\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Class probabilities: {probabilities}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Using Attention mechanism "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionDNASequenceRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A Recurrent Neural Network (RNN) with attention mechanism for DNA sequence classification.\n",
        "\n",
        "    This model uses an embedding layer to encode DNA sequences, a GRU (Gated Recurrent Unit)\n",
        "    for sequence modeling, and an attention mechanism to focus on important parts of the sequence.\n",
        "    The final output is passed through a fully connected layer for classification.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): The size of the input vocabulary (e.g., number of unique nucleotides).\n",
        "        embedding_dim (int): The dimension of the embedding space.\n",
        "        hidden_dim (int): The number of features in the hidden state of the GRU.\n",
        "        output_size (int): The number of output classes.\n",
        "        num_layers (int, optional): The number of GRU layers. Default is 1.\n",
        "        dropout (float, optional): Dropout probability for regularization. Default is 0.2.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers=1, dropout=0.2):\n",
        "        super(AttentionDNASequenceRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer to encode DNA sequences into dense vectors\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "\n",
        "        # GRU layer for sequence modeling\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=embedding_dim,  # Input dimension is the embedding dimension\n",
        "            hidden_size=hidden_dim,   # Hidden state dimension\n",
        "            num_layers=num_layers,    # Number of GRU layers\n",
        "            batch_first=True,         # Input and output tensors are (batch_size, seq_len, feature_dim)\n",
        "            dropout=dropout if num_layers > 1 else 0  # Apply dropout only if num_layers > 1\n",
        "        )\n",
        "\n",
        "        # Attention layer to compute attention scores\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the AttentionDNASequenceRNN.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len), where each element\n",
        "                              is an index corresponding to a nucleotide.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, output_size), containing the\n",
        "                          predicted class scores for each sequence in the batch.\n",
        "        \"\"\"\n",
        "        # Embed nucleotides into dense vectors\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Initialize the hidden state for the GRU\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Pass the embedded sequences through the GRU\n",
        "        out, _ = self.rnn(x, h0)  # out: (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # Compute attention scores for each time step\n",
        "        attention_scores = self.attention(out).squeeze(-1)  # (batch_size, seq_len)\n",
        "\n",
        "        # Normalize attention scores to get attention weights\n",
        "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=1)\n",
        "\n",
        "        # Apply attention weights to the GRU outputs\n",
        "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
        "        weighted_output = out * attention_weights  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # Compute the context vector as the weighted sum of GRU outputs\n",
        "        context_vector = weighted_output.sum(dim=1)  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        context_vector = self.dropout(context_vector)\n",
        "\n",
        "        # Pass the context vector through the fully connected layer for classification\n",
        "        output = self.fc(context_vector)\n",
        "\n",
        "        return output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGBAGy9lJ7x_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmZLKVb7UCAi"
   },
   "source": [
    "## Preparing DNA Sequence Data\n",
    "\n",
    "### Step 1: Load and explore your dataset\n",
    "\n",
    "Let's assume you have a dataset of DNA sequences and their corresponding labels. We'll create a PyTorch dataset class to handle this data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k5UnRdPRCcT"
   },
   "outputs": [],
   "source": [
    "class DNASequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "        # Define nucleotide mapping\n",
    "        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert sequence to numerical representation\n",
    "        sequence_encoded = [self.nucleotide_to_idx[nucleotide] for nucleotide in sequence]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sequence_tensor = torch.tensor(sequence_encoded, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sequence_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1IcUHK3ULU8"
   },
   "source": [
    "### Step 2: Load your dataset and split into train/validation/test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA5uJkcyRChD"
   },
   "outputs": [],
   "source": [
    "# Example data loading (replace with your actual data loading code)\n",
    "def load_dna_data():\n",
    "\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/master/practicum/Classification/acceptor_sites_dataset_train.csv\")\n",
    "    sequences = df['sequence'].values\n",
    "    labels = df['label'].values\n",
    "    labels = [0 if label == -1 else 1 for label in labels]\n",
    "    return sequences, labels\n",
    "\n",
    "# Load data\n",
    "sequences, labels = load_dna_data()\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "seqs_train, seqs_temp, labels_train, labels_temp = train_test_split(\n",
    "    sequences, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "seqs_val, seqs_test, labels_val, labels_test = train_test_split(\n",
    "    seqs_temp, labels_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(seqs_train)}\")\n",
    "print(f\"Validation samples: {len(seqs_val)}\")\n",
    "print(f\"Testing samples: {len(seqs_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8caDj6aUPUO"
   },
   "source": [
    "### Step 3: Create data loaders\n",
    "\n",
    "Question: How do we deal with different sequence lengths in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kU8CYMSRCnm"
   },
   "outputs": [],
   "source": [
    "# We need to pad sequences to the same length for batch processing\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # Find the length of the longest sequence\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        padded_seq = torch.zeros(max_length, dtype=torch.long)\n",
    "        padded_seq[:len(seq)] = seq\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    # Stack sequences and labels into batches\n",
    "    sequences_batch = torch.stack(padded_sequences)\n",
    "    labels_batch = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return sequences_batch, labels_batch\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DNASequenceDataset(seqs_train, labels_train)\n",
    "val_dataset = DNASequenceDataset(seqs_val, labels_val)\n",
    "test_dataset = DNASequenceDataset(seqs_test, labels_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faXV2ljsUSwo"
   },
   "source": [
    "## Building the RNN Model\n",
    "\n",
    "Now, let's define our RNN model for DNA sequence classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j06LevdRRRHG"
   },
   "outputs": [],
   "source": [
    "class DNASequenceRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers=1, dropout=0.2):\n",
    "        super(DNASequenceRNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer to convert nucleotide indices to dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # RNN layer (can be vanilla RNN, LSTM, or GRU)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "\n",
    "        # Embed nucleotides\n",
    "        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Pass through RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: (batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "        # We only need the output from the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # (batch_size, output_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_OZNu-fUV2F"
   },
   "source": [
    "### Step 4: Initialize model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o9_RNR-RRLL"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = 4  # Number of nucleotides (A, C, G, T)\n",
    "embedding_dim = 8  # Size of embedding vectors\n",
    "hidden_dim = 64  # Size of hidden layer\n",
    "output_size = 2  # Number of classes (binary classification)\n",
    "num_layers = 2  # Number of RNN layers\n",
    "dropout = 0.2  # Dropout rate\n",
    "\n",
    "# Create model\n",
    "model = DNASequenceRNN(input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M9dHN8iUYwl"
   },
   "source": [
    "## Training the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDxnVR4ERTW9"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train_losses, val_losses, val_accuracies = train(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWub8cmeUeL2"
   },
   "source": [
    "## Evaluating and Testing\n",
    "\n",
    "After training, let's evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ekU8HY0RYvE"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "\n",
    "    return test_loss, accuracy, report\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy, test_report = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj_JgncBUg_s"
   },
   "source": [
    "\n",
    "### Step 5: Making predictions with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prhHtwk6RY1h"
   },
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert sequence to indices\n",
    "    nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    sequence_encoded = [nucleotide_to_idx[nucleotide] for nucleotide in sequence]\n",
    "    sequence_tensor = torch.tensor(sequence_encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        # Get probability scores\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "\n",
    "    return predicted.item(), probabilities.cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "example_sequence = \"ATCGATCGATCGATCG\"\n",
    "predicted_class, probabilities = predict_sequence(model, example_sequence, device)\n",
    "\n",
    "print(f\"Sequence: {example_sequence}\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Using Attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDNASequenceRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers=1, dropout=0.2):\n",
    "        super(AttentionDNASequenceRNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "        # GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed nucleotides\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Pass through RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_scores = self.attention(out).squeeze(-1)  # (batch_size, seq_len)\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Apply attention weights\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "        weighted_output = out * attention_weights  # (batch_size, seq_len, hidden_dim)\n",
    "        context_vector = weighted_output.sum(dim=1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Apply dropout\n",
    "        context_vector = self.dropout(context_vector)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(context_vector)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
